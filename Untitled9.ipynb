{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nida07/college-chatbot-RNN/blob/master/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PI-XkJaWP0EQ"
      },
      "outputs": [],
      "source": [
        "pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAe__u-jwfxv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIaXbOA7JvaE"
      },
      "outputs": [],
      "source": [
        "#tensorflow is an open source for the ml framework, helping in training, building the model which provide API keras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQL18WWPP4LY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('/content/intents.json', 'r') as file:\n",
        "    intents = json.load(file)\n",
        "\n",
        "# Preprocess data\n",
        "patterns = []\n",
        "responses = []\n",
        "tags = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        patterns.append(pattern)\n",
        "        responses.append(intent['responses'][0])  # We'll just use the first response for simplicity\n",
        "        tags.append(intent['tag'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04E_6UdXQ6N_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# padding sequences to a fixed length, tokenizing text data\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize input patterns\n",
        "tokenizer = Tokenizer()\n",
        "#vocabulary, input patterns, and numerical indices based on word repetition\n",
        "tokenizer.fit_on_texts(patterns)\n",
        "#  word index dictionary from the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(patterns)\n",
        "# max number of sequences in each length and find the maximum and it set as pad_sequences\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "\n",
        "# Pad sequences to have same length add 0 after ie; padding = 'post' so each sequence will be same length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qSGDTQZRSnk"
      },
      "outputs": [],
      "source": [
        "# # LabelEncoder is used to convert categorical labels into numerical labels.\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Convert tags to numerical labels\n",
        "# label_encoder = LabelEncoder()\n",
        "# encoded_tags = label_encoder.fit_transform(tags)\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(padded_sequences, encoded_tags, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSK6ZPQAEG8c"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2KEdyFeQ_Vo",
        "outputId": "b364f42f-b922-48b8-c36d-83d5fe77fa96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 3s 17ms/step - loss: 5.9522 - accuracy: 0.0469\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 4.9951 - accuracy: 0.0667\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 3.6512 - accuracy: 0.0593\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5496 - accuracy: 0.0667\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5271 - accuracy: 0.0494\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 3.5164 - accuracy: 0.0642\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5231 - accuracy: 0.0568\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5225 - accuracy: 0.0420\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 3.5110 - accuracy: 0.0667\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5181 - accuracy: 0.0519\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5175 - accuracy: 0.0593\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5117 - accuracy: 0.0642\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5143 - accuracy: 0.0617\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5068 - accuracy: 0.0667\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 3.5136 - accuracy: 0.0494\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5091 - accuracy: 0.0593\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5080 - accuracy: 0.0593\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 3.5088 - accuracy: 0.0543\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5054 - accuracy: 0.0543\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5092 - accuracy: 0.0593\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 3.5088 - accuracy: 0.0642\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5011 - accuracy: 0.0691\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.5089 - accuracy: 0.0444\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 3.4919 - accuracy: 0.0642\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 3.4897 - accuracy: 0.0667\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 3.4768 - accuracy: 0.0543\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.4525 - accuracy: 0.0469\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 3.4167 - accuracy: 0.0691\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 3.3457 - accuracy: 0.0815\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 3.1926 - accuracy: 0.1111\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 3.0521 - accuracy: 0.1086\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 2.9082 - accuracy: 0.1284\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 2.7965 - accuracy: 0.1778\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 2.7020 - accuracy: 0.1679\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 2.6406 - accuracy: 0.1654\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 2.5654 - accuracy: 0.1926\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 22ms/step - loss: 2.5041 - accuracy: 0.2049\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 2.4562 - accuracy: 0.1901\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 2.3866 - accuracy: 0.1926\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 2.3077 - accuracy: 0.2420\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 2.2467 - accuracy: 0.2642\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 34ms/step - loss: 2.2148 - accuracy: 0.2198\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 2.1686 - accuracy: 0.2642\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 2.1170 - accuracy: 0.2914\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 33ms/step - loss: 2.0380 - accuracy: 0.3358\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 1.9572 - accuracy: 0.3802\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.9056 - accuracy: 0.3407\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 25ms/step - loss: 1.8423 - accuracy: 0.3877\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 1.7861 - accuracy: 0.3506\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.7077 - accuracy: 0.4123\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.6716 - accuracy: 0.4321\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 1.6178 - accuracy: 0.4765\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.5619 - accuracy: 0.4667\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.5170 - accuracy: 0.4864\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.4574 - accuracy: 0.5531\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 1.4590 - accuracy: 0.5062\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.4276 - accuracy: 0.5136\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.3368 - accuracy: 0.6025\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.3103 - accuracy: 0.6000\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.2477 - accuracy: 0.5852\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 1.2064 - accuracy: 0.6222\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.1917 - accuracy: 0.6025\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 1.1343 - accuracy: 0.6370\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 16ms/step - loss: 1.0948 - accuracy: 0.6346\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 1.0660 - accuracy: 0.6840\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 1.0131 - accuracy: 0.6938\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.9607 - accuracy: 0.7284\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.9304 - accuracy: 0.7654\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.8753 - accuracy: 0.7506\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.8235 - accuracy: 0.8025\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.7870 - accuracy: 0.8025\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.7710 - accuracy: 0.8198\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.7407 - accuracy: 0.7926\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.6981 - accuracy: 0.8321\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.6639 - accuracy: 0.8593\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 24ms/step - loss: 0.6189 - accuracy: 0.8667\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.5739 - accuracy: 0.8840\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.5305 - accuracy: 0.9012\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 17ms/step - loss: 0.5268 - accuracy: 0.9062\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4858 - accuracy: 0.9136\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4696 - accuracy: 0.9136\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.4615 - accuracy: 0.9136\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4323 - accuracy: 0.9160\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.4000 - accuracy: 0.9235\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.3633 - accuracy: 0.9383\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.3455 - accuracy: 0.9383\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.3170 - accuracy: 0.9506\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2985 - accuracy: 0.9580\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2807 - accuracy: 0.9630\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.2692 - accuracy: 0.9531\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.2507 - accuracy: 0.9704\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 35ms/step - loss: 0.2360 - accuracy: 0.9654\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 1s 47ms/step - loss: 0.2248 - accuracy: 0.9704\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.2129 - accuracy: 0.9753\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.2020 - accuracy: 0.9778\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.1887 - accuracy: 0.9753\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.1781 - accuracy: 0.9827\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.1682 - accuracy: 0.9852\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 23ms/step - loss: 0.1586 - accuracy: 0.9852\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 26ms/step - loss: 0.1517 - accuracy: 0.9852\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f8d7d6e5480>"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sequential is a class for creating a linear stack of layers. Embedding, LSTM,\n",
        "# and Dense are classes representing the layers\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# layers 3\n",
        "# Embedding(input dim, output dimension, input_length)\n",
        "# lstm uses 128 memory cell\n",
        "# dense (tot no of tags , activation function)\n",
        "\n",
        "\n",
        "# Define model architecture\n",
        "model = Sequential([\n",
        "    Embedding(len(word_index) + 1, 64, input_length=max_sequence_len),\n",
        "    LSTM(128),\n",
        "    Dense(len(tags), activation='softmax')\n",
        "])\n",
        "\n",
        "# adaptive moment estimation increase learning rate\n",
        "# loss is multiclass integer-based, optimizer update model weight, accuracy evaluation\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model(input,target label,epoch verbos)\n",
        "model.fit(padded_sequences, encoded_tags, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QiHpV6_Eqot"
      },
      "outputs": [],
      "source": [
        "# model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq5tZiv4UOY-"
      },
      "outputs": [],
      "source": [
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Convert tags to numerical labels\n",
        "# label_encoder = LabelEncoder()\n",
        "# encoded_tags = label_encoder.fit_transform(tags)\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(padded_sequences, encoded_tags, epochs=100, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP9VIoP6U5Ft"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def chat():\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"c u later!!\")\n",
        "            break\n",
        "\n",
        "        # Tokenize user input\n",
        "        input_seq = tokenizer.texts_to_sequences([user_input])\n",
        "        padded_input_seq = pad_sequences(input_seq, maxlen=max_sequence_len, padding='post')\n",
        "\n",
        "        # Predict probabilities for each class\n",
        "        predicted_probs = model.predict(padded_input_seq)\n",
        "\n",
        "        # Get predicted tag\n",
        "        predicted_tag_index = np.argmax(predicted_probs)\n",
        "        predicted_tag = label_encoder.inverse_transform([predicted_tag_index])[0]\n",
        "\n",
        "        # Get response\n",
        "        responses_list = [responses[i] for i, tag in enumerate(tags) if tag == predicted_tag]\n",
        "        print(\"Bot:\", np.random.choice(responses_list))\n",
        "\n",
        "# Start chatting\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyVjyEs8u_ge",
        "outputId": "b45f930a-f6c9-47fc-a7fc-4abf2b5ab5db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "81/81 [==============================] - 1s 2ms/step - loss: 3.5397 - accuracy: 0.1451\n",
            "Epoch 2/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 3.1552 - accuracy: 0.3549\n",
            "Epoch 3/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 2.6483 - accuracy: 0.4228\n",
            "Epoch 4/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 2.1117 - accuracy: 0.6728\n",
            "Epoch 5/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.6084 - accuracy: 0.8333\n",
            "Epoch 6/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 1.1894 - accuracy: 0.8981\n",
            "Epoch 7/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.8582 - accuracy: 0.9383\n",
            "Epoch 8/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.6212 - accuracy: 0.9630\n",
            "Epoch 9/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.9753\n",
            "Epoch 10/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.9815\n",
            "Epoch 11/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.2603 - accuracy: 0.9877\n",
            "Epoch 12/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.2029 - accuracy: 0.9877\n",
            "Epoch 13/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9938\n",
            "Epoch 14/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1325 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.1100 - accuracy: 0.9938\n",
            "Epoch 16/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0939 - accuracy: 0.9969\n",
            "Epoch 17/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0697 - accuracy: 0.9969\n",
            "Epoch 19/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0608 - accuracy: 0.9969\n",
            "Epoch 20/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0544 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0484 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0394 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0359 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0026 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "81/81 [==============================] - 0s 4ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "81/81 [==============================] - 0s 5ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 9.7325e-04 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 9.2266e-04 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "81/81 [==============================] - 0s 3ms/step - loss: 8.8007e-04 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 8.3495e-04 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 7.9263e-04 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 7.5905e-04 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 7.1882e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 6.8570e-04 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 6.5431e-04 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 6.2463e-04 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.9069e-04 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.7184e-04 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.4020e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 5.1532e-04 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 4.9216e-04 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 4.6627e-04 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "81/81 [==============================] - 0s 2ms/step - loss: 4.4749e-04 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a06a1c56050>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load data from intents.json\n",
        "with open('intents.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract patterns and tags from the dataset\n",
        "patterns = []\n",
        "tags = []\n",
        "responses = []\n",
        "for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        patterns.append(pattern)\n",
        "        tags.append(intent['tag'])\n",
        "        responses.append(intent['responses'][0])  # Assuming we'll use the first response\n",
        "\n",
        "# Tokenize input patterns\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(patterns).toarray()\n",
        "\n",
        "# Encode tags into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(tags)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build and train the model\n",
        "model = Sequential([\n",
        "    Dense(128, input_shape=(X_train.shape[1],), activation='relu'),\n",
        "    Dense(len(set(y)), activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=4, verbose=1)\n",
        "\n",
        "# # Function to interact with the chatbot\n",
        "# def chat():\n",
        "#     while True:\n",
        "#         user_input = input(\"You: \")\n",
        "#         if user_input.lower() == 'quit':\n",
        "#             break\n",
        "\n",
        "#         # Tokenize user input\n",
        "#         user_input_vec = vectorizer.transform([user_input]).toarray()\n",
        "\n",
        "#         # Predict tag\n",
        "#         predicted_tag = model.predict_classes(user_input_vec)\n",
        "\n",
        "#         # Get response\n",
        "#         response_index = np.where(y_train == predicted_tag)[0][0]\n",
        "#         print(\"Bot:\", responses[response_index])\n",
        "\n",
        "# # Start chatting\n",
        "# chat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jE1cBF9OxSRH"
      },
      "outputs": [],
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat():\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        # Tokenize user input\n",
        "        user_input_vec = vectorizer.transform([user_input]).toarray()\n",
        "\n",
        "        # Predict probabilities for each class\n",
        "        predicted_probs = model.predict(user_input_vec)\n",
        "\n",
        "        # Get the index of the class with the highest probability\n",
        "        predicted_tag_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get response\n",
        "        response_index = np.where(y_train == predicted_tag_index)[0][0]\n",
        "        print(\"Bot:\", responses[response_index])\n",
        "\n",
        "# Start chatting\n",
        "chat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXYWL6oFx1kP"
      },
      "outputs": [],
      "source": [
        "# Function to interact with the chatbot\n",
        "def chat():\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "        elif user_input.strip() == '':\n",
        "            print(\"Bot: Please provide input.\")\n",
        "            continue\n",
        "\n",
        "        # Tokenize user input\n",
        "        user_input_vec = vectorizer.transform([user_input]).toarray()\n",
        "\n",
        "        # Predict probabilities for each class\n",
        "        predicted_probs = model.predict(user_input_vec)\n",
        "\n",
        "        # Get the index of the class with the highest probability\n",
        "        predicted_tag_index = np.argmax(predicted_probs)\n",
        "\n",
        "        # Get response\n",
        "        if predicted_tag_index < len(responses):  # Check if index is within range\n",
        "            response_index = np.where(y_train == predicted_tag_index)[0][0]\n",
        "            print(\"Bot:\", responses[response_index])\n",
        "        else:\n",
        "            print(\"Bot: I'm sorry, I didn't understand that.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDEehLJNx5Dn",
        "outputId": "94662e98-2714-49f2-fabb-b12692f71045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "Bot: Hello!\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "Bot: Sad to see you go :(\n",
            "Bot: Please provide input.\n",
            "You: quit\n"
          ]
        }
      ],
      "source": [
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCD0f3s3wjJH"
      },
      "source": [
        "**CHAT BOT LSTM CHATTERBOT DATASET**\n",
        "\n",
        "*:::::::::::::::::::::::::::::::::::::\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BfIlkBew4y2"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVZ8Itexx_Yk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers, activations, models, preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xP8m4FtyEDo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import preprocessing, utils\n",
        "import os\n",
        "import yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_Int8a-yKQE"
      },
      "outputs": [],
      "source": [
        "# dir_path = '/kaggle/input/chatterbotenglish/'\n",
        "# files_list = os.listdir(dir_path + os.sep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKi7OP1n54xl"
      },
      "outputs": [],
      "source": [
        "import numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERs49YFAxsR0"
      },
      "source": [
        "new idea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fwK3ZlaxxUk",
        "outputId": "03df0bbc-f94c-45c2-908e-4799e17c10c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_PVepKix2jB",
        "outputId": "72c42085-1529-4ac6-97e4-1dc538c823e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras_tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras_tuner) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install keras_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4JJYFgryICn"
      },
      "outputs": [],
      "source": [
        "intents = json.loads(open('/content/intents.json').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA2WGY7-yQ4c"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "\n",
        "ignore_chars = ['?',',','!','.']\n",
        "\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        word = nltk.word_tokenize(pattern)\n",
        "        words.extend(word)\n",
        "        documents.append((word,intent[\"tag\"]))\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_duVfQDZyTLb",
        "outputId": "1cb2eec8-72ac-4792-fdb4-ba9b8e507cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['greeting',\n",
              " 'goodbye',\n",
              " 'creator',\n",
              " 'name',\n",
              " 'hours',\n",
              " 'number',\n",
              " 'course',\n",
              " 'fees',\n",
              " 'location',\n",
              " 'hostel',\n",
              " 'event',\n",
              " 'document',\n",
              " 'floors',\n",
              " 'syllabus',\n",
              " 'library',\n",
              " 'infrastructure',\n",
              " 'canteen',\n",
              " 'menu',\n",
              " 'placement',\n",
              " 'ithod',\n",
              " 'computerhod',\n",
              " 'extchod',\n",
              " 'principal',\n",
              " 'sem',\n",
              " 'admission',\n",
              " 'scholarship',\n",
              " 'facilities',\n",
              " 'college intake',\n",
              " 'uniform',\n",
              " 'committee',\n",
              " 'random',\n",
              " 'swear',\n",
              " 'vacation',\n",
              " 'sports',\n",
              " 'salutaion',\n",
              " 'task',\n",
              " 'ragging',\n",
              " 'hod']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jizQsCHkycxu",
        "outputId": "311775ce-4017-40bc-9de4-273fb36e4987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(['Hi'], 'greeting'),\n",
              " (['How', 'are', 'you', '?'], 'greeting'),\n",
              " (['Is', 'anyone', 'there', '?'], 'greeting'),\n",
              " (['Hello'], 'greeting'),\n",
              " (['Good', 'day'], 'greeting'),\n",
              " (['What', \"'s\", 'up'], 'greeting'),\n",
              " (['how', 'are', 'ya'], 'greeting'),\n",
              " (['heyy'], 'greeting'),\n",
              " (['whatsup'], 'greeting'),\n",
              " (['?', '?', '?', '?', '?', '?', '?', '?'], 'greeting'),\n",
              " (['cya'], 'goodbye'),\n",
              " (['see', 'you'], 'goodbye'),\n",
              " (['bye', 'bye'], 'goodbye'),\n",
              " (['See', 'you', 'later'], 'goodbye'),\n",
              " (['Goodbye'], 'goodbye'),\n",
              " (['I', 'am', 'Leaving'], 'goodbye'),\n",
              " (['Bye'], 'goodbye'),\n",
              " (['Have', 'a', 'Good', 'day'], 'goodbye'),\n",
              " (['talk', 'to', 'you', 'later'], 'goodbye'),\n",
              " (['ttyl'], 'goodbye'),\n",
              " (['i', 'got', 'to', 'go'], 'goodbye'),\n",
              " (['gtg'], 'goodbye'),\n",
              " (['what', 'is', 'the', 'name', 'of', 'your', 'developers'], 'creator'),\n",
              " (['what', 'is', 'the', 'name', 'of', 'your', 'creators'], 'creator'),\n",
              " (['what', 'is', 'the', 'name', 'of', 'the', 'developers'], 'creator'),\n",
              " (['what', 'is', 'the', 'name', 'of', 'the', 'creators'], 'creator'),\n",
              " (['who', 'created', 'you'], 'creator'),\n",
              " (['your', 'developers'], 'creator'),\n",
              " (['your', 'creators'], 'creator'),\n",
              " (['who', 'are', 'your', 'developers'], 'creator'),\n",
              " (['developers'], 'creator'),\n",
              " (['you', 'are', 'made', 'by'], 'creator'),\n",
              " (['you', 'are', 'made', 'by', 'whom'], 'creator'),\n",
              " (['who', 'created', 'you'], 'creator'),\n",
              " (['who', 'create', 'you'], 'creator'),\n",
              " (['creators'], 'creator'),\n",
              " (['who', 'made', 'you'], 'creator'),\n",
              " (['who', 'designed', 'you'], 'creator'),\n",
              " (['name'], 'name'),\n",
              " (['your', 'name'], 'name'),\n",
              " (['do', 'you', 'have', 'a', 'name'], 'name'),\n",
              " (['what', 'are', 'you', 'called'], 'name'),\n",
              " (['what', 'is', 'your', 'name'], 'name'),\n",
              " (['what', 'should', 'I', 'call', 'you'], 'name'),\n",
              " (['whats', 'your', 'name', '?'], 'name'),\n",
              " (['what', 'are', 'you'], 'name'),\n",
              " (['who', 'are', 'you'], 'name'),\n",
              " (['who', 'is', 'this'], 'name'),\n",
              " (['what', 'am', 'i', 'chatting', 'to'], 'name'),\n",
              " (['who', 'am', 'i', 'taking', 'to'], 'name'),\n",
              " (['what', 'are', 'you'], 'name'),\n",
              " (['timing', 'of', 'college'], 'hours'),\n",
              " (['what', 'is', 'college', 'timing'], 'hours'),\n",
              " (['working', 'days'], 'hours'),\n",
              " (['when', 'are', 'you', 'guys', 'open'], 'hours'),\n",
              " (['what', 'are', 'your', 'hours'], 'hours'),\n",
              " (['hours', 'of', 'operation'], 'hours'),\n",
              " (['when', 'is', 'the', 'college', 'open'], 'hours'),\n",
              " (['college', 'timing'], 'hours'),\n",
              " (['what', 'about', 'college', 'timing'], 'hours'),\n",
              " (['is', 'college', 'open', 'on', 'saturday'], 'hours'),\n",
              " (['tell', 'something', 'about', 'college', 'timing'], 'hours'),\n",
              " (['what', 'is', 'the', 'college', 'hours'], 'hours'),\n",
              " (['when', 'should', 'i', 'come', 'to', 'college'], 'hours'),\n",
              " (['when', 'should', 'i', 'attend', 'college'], 'hours'),\n",
              " (['what', 'is', 'my', 'college', 'time'], 'hours'),\n",
              " (['college', 'timing'], 'hours'),\n",
              " (['timing', 'college'], 'hours'),\n",
              " (['more', 'info'], 'number'),\n",
              " (['contact', 'info'], 'number'),\n",
              " (['how', 'to', 'contact', 'college'], 'number'),\n",
              " (['college', 'telephone', 'number'], 'number'),\n",
              " (['college', 'number'], 'number'),\n",
              " (['What', 'is', 'your', 'contact', 'no'], 'number'),\n",
              " (['Contact', 'number', '?'], 'number'),\n",
              " (['how', 'to', 'call', 'you'], 'number'),\n",
              " (['College', 'phone', 'no', '?'], 'number'),\n",
              " (['how', 'can', 'i', 'contact', 'you'], 'number'),\n",
              " (['Can', 'i', 'get', 'your', 'phone', 'number'], 'number'),\n",
              " (['how', 'can', 'i', 'call', 'you'], 'number'),\n",
              " (['phone', 'number'], 'number'),\n",
              " (['phone', 'no'], 'number'),\n",
              " (['call'], 'number'),\n",
              " (['list', 'of', 'courses'], 'course'),\n",
              " (['list', 'of', 'courses', 'offered'], 'course'),\n",
              " (['list', 'of', 'courses', 'offered', 'in'], 'course'),\n",
              " (['what', 'are', 'the', 'courses', 'offered', 'in', 'your', 'college', '?'],\n",
              "  'course'),\n",
              " (['courses', '?'], 'course'),\n",
              " (['courses', 'offered'], 'course'),\n",
              " (['courses',\n",
              "   'offered',\n",
              "   'in',\n",
              "   '(',\n",
              "   'your',\n",
              "   'univrsity',\n",
              "   '(',\n",
              "   'UNI',\n",
              "   ')',\n",
              "   'name',\n",
              "   ')'],\n",
              "  'course'),\n",
              " (['courses', 'you', 'offer'], 'course'),\n",
              " (['branches', '?'], 'course'),\n",
              " (['courses', 'available', 'at', 'UNI', '?'], 'course'),\n",
              " (['branches', 'available', 'at', 'your', 'college', '?'], 'course'),\n",
              " (['what', 'are', 'the', 'courses', 'in', 'UNI', '?'], 'course'),\n",
              " (['what', 'are', 'branches', 'in', 'UNI', '?'], 'course'),\n",
              " (['what', 'are', 'courses', 'in', 'UNI', '?'], 'course'),\n",
              " (['branches', 'available', 'in', 'UNI', '?'], 'course'),\n",
              " (['can',\n",
              "   'you',\n",
              "   'tell',\n",
              "   'me',\n",
              "   'the',\n",
              "   'courses',\n",
              "   'available',\n",
              "   'in',\n",
              "   'UNI',\n",
              "   '?'],\n",
              "  'course'),\n",
              " (['can',\n",
              "   'you',\n",
              "   'tell',\n",
              "   'me',\n",
              "   'the',\n",
              "   'branches',\n",
              "   'available',\n",
              "   'in',\n",
              "   'UNI',\n",
              "   '?'],\n",
              "  'course'),\n",
              " (['computer', 'engineering', '?'], 'course'),\n",
              " (['computer'], 'course'),\n",
              " (['Computer', 'engineering', '?'], 'course'),\n",
              " (['it'], 'course'),\n",
              " (['IT'], 'course'),\n",
              " (['Information', 'Technology'], 'course'),\n",
              " (['AI/Ml'], 'course'),\n",
              " (['Mechanical', 'engineering'], 'course'),\n",
              " (['Chemical', 'engineering'], 'course'),\n",
              " (['Civil', 'engineering'], 'course'),\n",
              " (['information', 'about', 'fee'], 'fees'),\n",
              " (['information', 'on', 'fee'], 'fees'),\n",
              " (['tell', 'me', 'the', 'fee'], 'fees'),\n",
              " (['college', 'fee'], 'fees'),\n",
              " (['fee', 'per', 'semester'], 'fees'),\n",
              " (['what', 'is', 'the', 'fee', 'of', 'each', 'semester'], 'fees'),\n",
              " (['what', 'is', 'the', 'fees', 'of', 'each', 'year'], 'fees'),\n",
              " (['what', 'is', 'fee'], 'fees'),\n",
              " (['what', 'is', 'the', 'fees'], 'fees'),\n",
              " (['how', 'much', 'is', 'the', 'fees'], 'fees'),\n",
              " (['fees', 'for', 'first', 'year'], 'fees'),\n",
              " (['fees'], 'fees'),\n",
              " (['about', 'the', 'fees'], 'fees'),\n",
              " (['tell', 'me', 'something', 'about', 'the', 'fees'], 'fees'),\n",
              " (['What', 'is', 'the', 'fees', 'of', 'hostel'], 'fees'),\n",
              " (['how', 'much', 'is', 'the', 'fees'], 'fees'),\n",
              " (['hostel', 'fees'], 'fees'),\n",
              " (['fees', 'for', 'AC', 'room'], 'fees'),\n",
              " (['fees', 'for', 'non-AC', 'room'], 'fees'),\n",
              " (['fees', 'for', 'Ac', 'room', 'for', 'girls'], 'fees'),\n",
              " (['fees', 'for', 'non-Ac', 'room', 'for', 'girls'], 'fees'),\n",
              " (['fees', 'for', 'Ac', 'room', 'for', 'boys'], 'fees'),\n",
              " (['fees', 'for', 'non-Ac', 'room', 'for', 'boys'], 'fees'),\n",
              " (['where', 'is', 'the', 'college', 'located'], 'location'),\n",
              " (['college', 'is', 'located', 'at'], 'location'),\n",
              " (['where', 'is', 'college'], 'location'),\n",
              " (['where', 'is', 'college', 'located'], 'location'),\n",
              " (['address', 'of', 'college'], 'location'),\n",
              " (['how', 'to', 'reach', 'college'], 'location'),\n",
              " (['college', 'location'], 'location'),\n",
              " (['college', 'address'], 'location'),\n",
              " (['wheres', 'the', 'college'], 'location'),\n",
              " (['how', 'can', 'I', 'reach', 'college'], 'location'),\n",
              " (['whats', 'is', 'the', 'college', 'address'], 'location'),\n",
              " (['what', 'is', 'the', 'address', 'of', 'college'], 'location'),\n",
              " (['address'], 'location'),\n",
              " (['location'], 'location'),\n",
              " (['hostel', 'facility'], 'hostel'),\n",
              " (['hostel', 'servive'], 'hostel'),\n",
              " (['hostel', 'location'], 'hostel'),\n",
              " (['hostel', 'address'], 'hostel'),\n",
              " (['hostel', 'facilities'], 'hostel'),\n",
              " (['hostel', 'fees'], 'hostel'),\n",
              " (['Does', 'college', 'provide', 'hostel'], 'hostel'),\n",
              " (['Is', 'there', 'any', 'hostel'], 'hostel'),\n",
              " (['Where', 'is', 'hostel'], 'hostel'),\n",
              " (['do', 'you', 'have', 'hostel'], 'hostel'),\n",
              " (['do', 'you', 'guys', 'have', 'hostel'], 'hostel'),\n",
              " (['hostel'], 'hostel'),\n",
              " (['hostel', 'capacity'], 'hostel'),\n",
              " (['what', 'is', 'the', 'hostel', 'fee'], 'hostel'),\n",
              " (['how', 'to', 'get', 'in', 'hostel'], 'hostel'),\n",
              " (['what', 'is', 'the', 'hostel', 'address'], 'hostel'),\n",
              " (['how', 'far', 'is', 'hostel', 'from', 'college'], 'hostel'),\n",
              " (['hostel', 'college', 'distance'], 'hostel'),\n",
              " (['where', 'is', 'the', 'hostel'], 'hostel'),\n",
              " (['how', 'big', 'is', 'the', 'hostel'], 'hostel'),\n",
              " (['distance', 'between', 'college', 'and', 'hostel'], 'hostel'),\n",
              " (['distance', 'between', 'hostel', 'and', 'college'], 'hostel'),\n",
              " (['events', 'organised'], 'event'),\n",
              " (['list', 'of', 'events'], 'event'),\n",
              " (['list', 'of', 'events', 'organised', 'in', 'college'], 'event'),\n",
              " (['list', 'of', 'events', 'conducted', 'in', 'college'], 'event'),\n",
              " (['What', 'events', 'are', 'conducted', 'in', 'college'], 'event'),\n",
              " (['Are', 'there', 'any', 'event', 'held', 'at', 'college'], 'event'),\n",
              " (['Events', '?'], 'event'),\n",
              " (['functions'], 'event'),\n",
              " (['what', 'are', 'the', 'events'], 'event'),\n",
              " (['tell', 'me', 'about', 'events'], 'event'),\n",
              " (['what', 'about', 'events'], 'event'),\n",
              " (['document', 'to', 'bring'], 'document'),\n",
              " (['documents', 'needed', 'for', 'admision'], 'document'),\n",
              " (['documents', 'needed', 'at', 'the', 'time', 'of', 'admission'], 'document'),\n",
              " (['documents', 'needed', 'during', 'admission'], 'document'),\n",
              " (['documents', 'required', 'for', 'admision'], 'document'),\n",
              " (['documents', 'required', 'at', 'the', 'time', 'of', 'admission'],\n",
              "  'document'),\n",
              " (['documents', 'required', 'during', 'admission'], 'document'),\n",
              " (['What', 'document', 'are', 'required', 'for', 'admission'], 'document'),\n",
              " (['Which', 'document', 'to', 'bring', 'for', 'admission'], 'document'),\n",
              " (['documents'], 'document'),\n",
              " (['what', 'documents', 'do', 'i', 'need'], 'document'),\n",
              " (['what', 'documents', 'do', 'I', 'need', 'for', 'admission'], 'document'),\n",
              " (['documents', 'needed'], 'document'),\n",
              " (['size', 'of', 'campus'], 'floors'),\n",
              " (['building', 'size'], 'floors'),\n",
              " (['How', 'many', 'floors', 'does', 'college', 'have'], 'floors'),\n",
              " (['floors', 'in', 'college'], 'floors'),\n",
              " (['floors', 'in', 'college'], 'floors'),\n",
              " (['how',\n",
              "   'tall',\n",
              "   'is',\n",
              "   'UNI',\n",
              "   \"'s\",\n",
              "   'College',\n",
              "   'of',\n",
              "   'Engineering',\n",
              "   'college',\n",
              "   'building'],\n",
              "  'floors'),\n",
              " (['floors'], 'floors'),\n",
              " (['Syllabus', 'for', 'IT'], 'syllabus'),\n",
              " (['what', 'is', 'the', 'Information', 'Technology', 'syllabus'], 'syllabus'),\n",
              " (['syllabus'], 'syllabus'),\n",
              " (['timetable'], 'syllabus'),\n",
              " (['what', 'is', 'IT', 'syllabus'], 'syllabus'),\n",
              " (['syllabus'], 'syllabus'),\n",
              " (['What', 'is', 'next', 'lecture'], 'syllabus'),\n",
              " (['is', 'there', 'any', 'library'], 'library'),\n",
              " (['library', 'facility'], 'library'),\n",
              " (['library', 'facilities'], 'library'),\n",
              " (['do', 'you', 'have', 'library'], 'library'),\n",
              " (['does', 'the', 'college', 'have', 'library', 'facility'], 'library'),\n",
              " (['college', 'library'], 'library'),\n",
              " (['where', 'can', 'i', 'get', 'books'], 'library'),\n",
              " (['book', 'facility'], 'library'),\n",
              " (['Where', 'is', 'library'], 'library'),\n",
              " (['Library'], 'library'),\n",
              " (['Library', 'information'], 'library'),\n",
              " (['Library', 'books', 'information'], 'library'),\n",
              " (['Tell', 'me', 'about', 'library'], 'library'),\n",
              " (['how', 'many', 'libraries'], 'library'),\n",
              " (['how', 'is', 'college', 'infrastructure'], 'infrastructure'),\n",
              " (['infrastructure'], 'infrastructure'),\n",
              " (['college', 'infrastructure'], 'infrastructure'),\n",
              " (['food', 'facilities'], 'canteen'),\n",
              " (['canteen', 'facilities'], 'canteen'),\n",
              " (['canteen', 'facility'], 'canteen'),\n",
              " (['is', 'there', 'any', 'canteen'], 'canteen'),\n",
              " (['Is', 'there', 'a', 'cafetaria', 'in', 'college'], 'canteen'),\n",
              " (['Does', 'college', 'have', 'canteen'], 'canteen'),\n",
              " (['Where', 'is', 'canteen'], 'canteen'),\n",
              " (['where', 'is', 'cafetaria'], 'canteen'),\n",
              " (['canteen'], 'canteen'),\n",
              " (['Food'], 'canteen'),\n",
              " (['Cafetaria'], 'canteen'),\n",
              " (['food', 'menu'], 'menu'),\n",
              " (['food', 'in', 'canteen'], 'menu'),\n",
              " (['Whats', 'there', 'on', 'menu'], 'menu'),\n",
              " (['what', 'is', 'available', 'in', 'college', 'canteen'], 'menu'),\n",
              " (['what', 'foods', 'can', 'we', 'get', 'in', 'college', 'canteen'], 'menu'),\n",
              " (['food', 'variety'], 'menu'),\n",
              " (['What', 'is', 'there', 'to', 'eat', '?'], 'menu'),\n",
              " (['What', 'is', 'college', 'placement'], 'placement'),\n",
              " (['Which', 'companies', 'visit', 'in', 'college'], 'placement'),\n",
              " (['What', 'is', 'average', 'package'], 'placement'),\n",
              " (['companies', 'visit'], 'placement'),\n",
              " (['package'], 'placement'),\n",
              " (['About', 'placement'], 'placement'),\n",
              " (['placement'], 'placement'),\n",
              " (['recruitment'], 'placement'),\n",
              " (['companies'], 'placement'),\n",
              " (['Who', 'is', 'HOD'], 'ithod'),\n",
              " (['Where', 'is', 'HOD'], 'ithod'),\n",
              " (['it', 'hod'], 'ithod'),\n",
              " (['name', 'of', 'it', 'hod'], 'ithod'),\n",
              " (['Who', 'is', 'computer', 'HOD'], 'computerhod'),\n",
              " (['Where', 'is', 'computer', 'HOD'], 'computerhod'),\n",
              " (['computer', 'hod'], 'computerhod'),\n",
              " (['name', 'of', 'computer', 'hod'], 'computerhod'),\n",
              " (['Who', 'is', 'extc', 'HOD'], 'extchod'),\n",
              " (['Where', 'is', 'extc', 'HOD'], 'extchod'),\n",
              " (['extc', 'hod'], 'extchod'),\n",
              " (['name', 'of', 'extc', 'hod'], 'extchod'),\n",
              " (['what', 'is', 'the', 'name', 'of', 'principal'], 'principal'),\n",
              " (['whatv', 'is', 'the', 'principal', 'name'], 'principal'),\n",
              " (['principal', 'name'], 'principal'),\n",
              " (['Who', 'is', 'college', 'principal'], 'principal'),\n",
              " (['Where', 'is', 'principal', \"'s\", 'office'], 'principal'),\n",
              " (['principal'], 'principal'),\n",
              " (['name', 'of', 'principal'], 'principal'),\n",
              " (['exam', 'dates'], 'sem'),\n",
              " (['exam', 'schedule'], 'sem'),\n",
              " (['When', 'is', 'semester', 'exam'], 'sem'),\n",
              " (['Semester', 'exam', 'timetable'], 'sem'),\n",
              " (['sem'], 'sem'),\n",
              " (['semester'], 'sem'),\n",
              " (['exam'], 'sem'),\n",
              " (['when', 'is', 'exam'], 'sem'),\n",
              " (['exam', 'timetable'], 'sem'),\n",
              " (['exam', 'dates'], 'sem'),\n",
              " (['when', 'is', 'semester'], 'sem'),\n",
              " (['what', 'is', 'the', 'process', 'of', 'admission'], 'admission'),\n",
              " (['what', 'is', 'the', 'admission', 'process'], 'admission'),\n",
              " (['How', 'to', 'take', 'admission', 'in', 'your', 'college'], 'admission'),\n",
              " (['What', 'is', 'the', 'process', 'for', 'admission'], 'admission'),\n",
              " (['admission'], 'admission'),\n",
              " (['admission', 'process'], 'admission'),\n",
              " (['scholarship'], 'scholarship'),\n",
              " (['Is', 'scholarship', 'available'], 'scholarship'),\n",
              " (['scholarship', 'engineering'], 'scholarship'),\n",
              " (['scholarship', 'it'], 'scholarship'),\n",
              " (['scholarship', 'ce'], 'scholarship'),\n",
              " (['scholarship', 'mechanical'], 'scholarship'),\n",
              " (['scholarship', 'civil'], 'scholarship'),\n",
              " (['scholarship', 'chemical'], 'scholarship'),\n",
              " (['scholarship', 'for', 'AI/ML'], 'scholarship'),\n",
              " (['available', 'scholarships'], 'scholarship'),\n",
              " (['scholarship', 'for', 'computer', 'engineering'], 'scholarship'),\n",
              " (['scholarship', 'for', 'IT', 'engineering'], 'scholarship'),\n",
              " (['scholarship', 'for', 'mechanical', 'engineering'], 'scholarship'),\n",
              " (['scholarship', 'for', 'civil', 'engineering'], 'scholarship'),\n",
              " (['scholarship', 'for', 'chemical', 'engineering'], 'scholarship'),\n",
              " (['list', 'of', 'scholarship'], 'scholarship'),\n",
              " (['comps', 'scholarship'], 'scholarship'),\n",
              " (['IT', 'scholarship'], 'scholarship'),\n",
              " (['mechanical', 'scholarship'], 'scholarship'),\n",
              " (['civil', 'scholarship'], 'scholarship'),\n",
              " (['chemical', 'scholarship'], 'scholarship'),\n",
              " (['automobile', 'scholarship'], 'scholarship'),\n",
              " (['first', 'year', 'scholarship'], 'scholarship'),\n",
              " (['second', 'year', 'scholarship'], 'scholarship'),\n",
              " (['third', 'year', 'scholarship'], 'scholarship'),\n",
              " (['fourth', 'year', 'scholarship'], 'scholarship'),\n",
              " (['What', 'facilities', 'college', 'provide'], 'facilities'),\n",
              " (['College', 'facility'], 'facilities'),\n",
              " (['What', 'are', 'college', 'facilities'], 'facilities'),\n",
              " (['facilities'], 'facilities'),\n",
              " (['facilities', 'provided'], 'facilities'),\n",
              " (['max', 'number', 'of', 'students'], 'college intake'),\n",
              " (['number', 'of', 'seats', 'per', 'branch'], 'college intake'),\n",
              " (['number', 'of', 'seats', 'in', 'each', 'branch'], 'college intake'),\n",
              " (['maximum', 'number', 'of', 'seats'], 'college intake'),\n",
              " (['maximum', 'students', 'intake'], 'college intake'),\n",
              " (['What', 'is', 'college', 'intake'], 'college intake'),\n",
              " (['how', 'many', 'stundent', 'are', 'taken', 'in', 'each', 'branch'],\n",
              "  'college intake'),\n",
              " (['seat', 'allotment'], 'college intake'),\n",
              " (['seats'], 'college intake'),\n",
              " (['college', 'dress', 'code'], 'uniform'),\n",
              " (['college', 'dresscode'], 'uniform'),\n",
              " (['what', 'is', 'the', 'uniform'], 'uniform'),\n",
              " (['can', 'we', 'wear', 'casuals'], 'uniform'),\n",
              " (['Does', 'college', 'have', 'an', 'uniform'], 'uniform'),\n",
              " (['Is', 'there', 'any', 'uniform'], 'uniform'),\n",
              " (['uniform'], 'uniform'),\n",
              " (['what', 'about', 'uniform'], 'uniform'),\n",
              " (['do', 'we', 'have', 'to', 'wear', 'uniform'], 'uniform'),\n",
              " (['what', 'are', 'the', 'different', 'committe', 'in', 'college'],\n",
              "  'committee'),\n",
              " (['different', 'committee', 'in', 'college'], 'committee'),\n",
              " (['Are', 'there', 'any', 'committee', 'in', 'college'], 'committee'),\n",
              " (['Give', 'me', 'committee', 'details'], 'committee'),\n",
              " (['committee'], 'committee'),\n",
              " (['how', 'many', 'committee', 'are', 'there', 'in', 'college'], 'committee'),\n",
              " (['I', 'love', 'you'], 'random'),\n",
              " (['Will', 'you', 'marry', 'me'], 'random'),\n",
              " (['Do', 'you', 'love', 'me'], 'random'),\n",
              " (['fuck'], 'swear'),\n",
              " (['bitch'], 'swear'),\n",
              " (['shut', 'up'], 'swear'),\n",
              " (['hell'], 'swear'),\n",
              " (['stupid'], 'swear'),\n",
              " (['idiot'], 'swear'),\n",
              " (['dumb', 'ass'], 'swear'),\n",
              " (['asshole'], 'swear'),\n",
              " (['fucker'], 'swear'),\n",
              " (['holidays'], 'vacation'),\n",
              " (['when', 'will', 'semester', 'starts'], 'vacation'),\n",
              " (['when', 'will', 'semester', 'end'], 'vacation'),\n",
              " (['when', 'is', 'the', 'holidays'], 'vacation'),\n",
              " (['list', 'of', 'holidays'], 'vacation'),\n",
              " (['Holiday', 'in', 'these', 'year'], 'vacation'),\n",
              " (['holiday', 'list'], 'vacation'),\n",
              " (['about', 'vacations'], 'vacation'),\n",
              " (['about', 'holidays'], 'vacation'),\n",
              " (['When', 'is', 'vacation'], 'vacation'),\n",
              " (['When', 'is', 'holidays'], 'vacation'),\n",
              " (['how', 'long', 'will', 'be', 'the', 'vacation'], 'vacation'),\n",
              " (['sports', 'and', 'games'], 'sports'),\n",
              " (['give', 'sports', 'details'], 'sports'),\n",
              " (['sports', 'infrastructure'], 'sports'),\n",
              " (['sports', 'facilities'], 'sports'),\n",
              " (['information', 'about', 'sports'], 'sports'),\n",
              " (['Sports', 'activities'], 'sports'),\n",
              " (['please', 'provide', 'sports', 'and', 'games', 'information'], 'sports'),\n",
              " (['okk'], 'salutaion'),\n",
              " (['okie'], 'salutaion'),\n",
              " (['nice', 'work'], 'salutaion'),\n",
              " (['well', 'done'], 'salutaion'),\n",
              " (['good', 'job'], 'salutaion'),\n",
              " (['thanks', 'for', 'the', 'help'], 'salutaion'),\n",
              " (['Thank', 'You'], 'salutaion'),\n",
              " (['its', 'ok'], 'salutaion'),\n",
              " (['Thanks'], 'salutaion'),\n",
              " (['Good', 'work'], 'salutaion'),\n",
              " (['k'], 'salutaion'),\n",
              " (['ok'], 'salutaion'),\n",
              " (['okay'], 'salutaion'),\n",
              " (['what', 'can', 'you', 'do'], 'task'),\n",
              " (['what', 'are', 'the', 'thing', 'you', 'can', 'do'], 'task'),\n",
              " (['things', 'you', 'can', 'do'], 'task'),\n",
              " (['what', 'can', 'u', 'do', 'for', 'me'], 'task'),\n",
              " (['how', 'u', 'can', 'help', 'me'], 'task'),\n",
              " (['why', 'i', 'should', 'use', 'you'], 'task'),\n",
              " (['ragging'], 'ragging'),\n",
              " (['is', 'ragging', 'practice', 'active', 'in', 'college'], 'ragging'),\n",
              " (['does', 'college', 'have', 'any', 'antiragging', 'facility'], 'ragging'),\n",
              " (['is', 'there', 'any', 'ragging', 'cases'], 'ragging'),\n",
              " (['is', 'ragging', 'done', 'here'], 'ragging'),\n",
              " (['ragging', 'against'], 'ragging'),\n",
              " (['antiragging', 'facility'], 'ragging'),\n",
              " (['ragging', 'juniors'], 'ragging'),\n",
              " (['ragging', 'history'], 'ragging'),\n",
              " (['ragging', 'incidents'], 'ragging'),\n",
              " (['hod'], 'hod'),\n",
              " (['hod', 'name'], 'hod'),\n",
              " (['who', 'is', 'the', 'hod'], 'hod')]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDLprqjxyktB"
      },
      "outputs": [],
      "source": [
        "stemmer = LancasterStemmer()\n",
        "words = [stemmer.stem(word.lower()) for word in words if word not in ignore_chars]\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfIdW0qiyo8A",
        "outputId": "72cefb54-3407-454e-cbda-d0f84fb9ab68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['admission',\n",
              " 'canteen',\n",
              " 'college intake',\n",
              " 'committee',\n",
              " 'computerhod',\n",
              " 'course',\n",
              " 'creator',\n",
              " 'document',\n",
              " 'event',\n",
              " 'extchod',\n",
              " 'facilities',\n",
              " 'fees',\n",
              " 'floors',\n",
              " 'goodbye',\n",
              " 'greeting',\n",
              " 'hod',\n",
              " 'hostel',\n",
              " 'hours',\n",
              " 'infrastructure',\n",
              " 'ithod',\n",
              " 'library',\n",
              " 'location',\n",
              " 'menu',\n",
              " 'name',\n",
              " 'number',\n",
              " 'placement',\n",
              " 'principal',\n",
              " 'ragging',\n",
              " 'random',\n",
              " 'salutaion',\n",
              " 'scholarship',\n",
              " 'sem',\n",
              " 'sports',\n",
              " 'swear',\n",
              " 'syllabus',\n",
              " 'task',\n",
              " 'uniform',\n",
              " 'vacation']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCvZycvIyssm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "training_data = []\n",
        "output_empty = [0] * len(classes)\n",
        "for doc in documents:\n",
        "  bagofwords = []\n",
        "  word_patterns = doc[0]\n",
        "  word_patterns = [stemmer.stem(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bagofwords.append(1) if word in word_patterns else bagofwords.append(0)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training_data.append([bagofwords, output_row])\n",
        "    random.shuffle(training_data)\n",
        "training_data = np.array(training_data)\n",
        "train_x = list(training_data[:,0])\n",
        "train_y = list(training_data[:,1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKBGGtheRQ8d1cVI6CRP7h",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}